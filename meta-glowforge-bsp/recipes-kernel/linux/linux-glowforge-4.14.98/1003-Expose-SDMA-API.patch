linux-glowforge: Expose SDMA API

Exposes SDMA API to userspace

Upstream-Status: Inappropriate [embedded specific]

Signed-off-by: Scott Wiederhold <s.e.wiederhold@gmail.com>

diff -u a/include/linux/platform_data/dma-imx-sdma.h b/include/linux/platform_data/dma-imx-sdma.h
--- a/include/linux/platform_data/dma-imx-sdma.h
+++ a/include/linux/platform_data/dma-imx-sdma.h
@@ -1,6 +1,21 @@
-/* SPDX-License-Identifier: GPL-2.0 */
+/**
+ * Copyright (C) 2018 Scott Wiederhold <s.e.wiederhold@gmail.com>
+ * Portions Copyright (C) 2015-2018 Glowforge, Inc. <opensource@glowforge.com>
+ * Portions Copyright 2004-2016 Freescale Semiconductor, Inc. All Rights Reserved.
+ * Portions Copyright 2010 Sascha Hauer, Pengutronix <s.hauer@pengutronix.de>
+ *
+ * The code contained herein is licensed under the GNU General Public
+ * License. You may obtain a copy of the GNU General Public License
+ * Version 2 or later at the following locations:
+ *
+ * http://www.opensource.org/licenses/gpl-license.html
+ * http://www.gnu.org/copyleft/gpl.html
+ */
+
 #ifndef __MACH_MXC_SDMA_H__
 #define __MACH_MXC_SDMA_H__
+
+#include <linux/dmaengine.h>

 /**
  * struct sdma_script_start_addrs - SDMA script start pointers
@@ -52,11 +67,7 @@
 	s32 zcanfd_2_mcu_addr;
 	s32 zqspi_2_mcu_addr;
 	s32 mcu_2_ecspi_addr;
-	s32 mcu_2_sai_addr;
-	s32 sai_2_mcu_addr;
 	/* End of v3 array */
-	s32 mcu_2_zqspi_addr;
-	/* End of v4 array */
 };

 /**
@@ -70,4 +81,277 @@
 	struct sdma_script_start_addrs *script_addrs;
 };

+/*
+ * Mode/Count of data node descriptors - IPCv2
+ */
+struct sdma_mode_count {
+	u32 count   : 16; /* size of the buffer pointed by this BD */
+	u32 status  :  8; /* E,R,I,C,W,D status bits stored here */
+	u32 command :  8; /* command mostlky used for channel 0 */
+};
+
+/*
+ * Buffer descriptor
+ */
+struct sdma_buffer_descriptor {
+	struct sdma_mode_count  mode;
+	u32 buffer_addr;	/* address of the buffer described */
+	u32 ext_buffer_addr;	/* extended buffer address */
+} __attribute__ ((packed));
+
+/**
+ * struct sdma_channel_control - Channel control Block
+ *
+ * @current_bd_ptr	current buffer descriptor processed
+ * @base_bd_ptr		first element of buffer descriptor array
+ * @unused		padding. The SDMA engine expects an array of 128 byte
+ *			control blocks
+ */
+struct sdma_channel_control {
+	u32 current_bd_ptr;
+	u32 base_bd_ptr;
+	u32 unused[2];
+} __attribute__ ((packed));
+
+/**
+ * struct sdma_state_registers - SDMA context for a channel
+ *
+ * @pc:		program counter
+ * @t:		test bit: status of arithmetic & test instruction
+ * @rpc:	return program counter
+ * @sf:		source fault while loading data
+ * @spc:	loop start program counter
+ * @df:		destination fault while storing data
+ * @epc:	loop end program counter
+ * @lm:		loop mode
+ */
+struct sdma_state_registers {
+	u32 pc     :14;
+	u32 unused1: 1;
+	u32 t      : 1;
+	u32 rpc    :14;
+	u32 unused0: 1;
+	u32 sf     : 1;
+	u32 spc    :14;
+	u32 unused2: 1;
+	u32 df     : 1;
+	u32 epc    :14;
+	u32 lm     : 2;
+} __attribute__ ((packed));
+
+/**
+ * struct sdma_context_data - sdma context specific to a channel
+ *
+ * @channel_state:	channel state bits
+ * @gReg:		general registers
+ * @mda:		burst dma destination address register
+ * @msa:		burst dma source address register
+ * @ms:			burst dma status register
+ * @md:			burst dma data register
+ * @pda:		peripheral dma destination address register
+ * @psa:		peripheral dma source address register
+ * @ps:			peripheral dma status register
+ * @pd:			peripheral dma data register
+ * @ca:			CRC polynomial register
+ * @cs:			CRC accumulator register
+ * @dda:		dedicated core destination address register
+ * @dsa:		dedicated core source address register
+ * @ds:			dedicated core status register
+ * @dd:			dedicated core data register
+ */
+struct sdma_context_data {
+	struct sdma_state_registers  channel_state;
+	u32  gReg[8];
+	u32  mda;
+	u32  msa;
+	u32  ms;
+	u32  md;
+	u32  pda;
+	u32  psa;
+	u32  ps;
+	u32  pd;
+	u32  ca;
+	u32  cs;
+	u32  dda;
+	u32  dsa;
+	u32  ds;
+	u32  dd;
+	u32  scratch0;
+	u32  scratch1;
+	u32  scratch2;
+	u32  scratch3;
+	u32  scratch4;
+	u32  scratch5;
+	u32  scratch6;
+	u32  scratch7;
+} __attribute__ ((packed));
+
+#define NUM_BD (int)(PAGE_SIZE / sizeof(struct sdma_buffer_descriptor))
+#define SDMA_BD_MAX_CNT	0xfffc /* align with 4 bytes */
+
+struct sdma_engine;
+
+struct sdma_channel;
+
+/**
+ * sdma_engine_get() - returns a pointer to the global SDMA engine
+ *
+ * Return: pointer to the sdma_engine object
+ */
+struct sdma_engine *sdma_engine_get(void);
+
+/**
+ * sdma_get_channel() - returns a pointer to the numbered SDMA channel
+ * @sdma:	pointer to the sdma_engine object
+ * @channel:	channel number from 0-31
+ *
+ * Return: pointer to channel object, or NULL
+ */
+struct sdma_channel *sdma_get_channel(struct sdma_engine *sdma, int channel);
+
+/**
+ * sdma_set_channel_interrupt_callback() - sets a custom interrupt handler
+ * @sdmac:	pointer to sdma_channel object
+ * @int_cb:	callback function to register
+ * @cb_param:	user-defined object passed when the callback is invoked
+ *
+ * Sets a function to be called when a custom SDMA script triggers an interrupt
+ * (e.g. with a "done 3" instruction).
+ * The function is executed in tasklet (atomic) context.
+ */
+void sdma_set_channel_interrupt_callback(struct sdma_channel *sdmac,
+		dma_async_tx_callback int_cb, void *cb_param);
+
+/**
+ * sdma_set_channel_priority() - sets the channel's execution priority
+ * @sdmac:	pointer to sdma_channel object
+ * @priority:	priority, from 0 (disabled) to 7 (highest)
+ *
+ * Setting a nonzero priority may cause the channel's script to begin executing,
+ * depending on how it is configured.
+ * Priority 7 is used by channel 0 for loading scripts/context. Typically,
+ * channel 0 should be the only channel with priority 7.
+ *
+ * Return: 0 on success, nonzero otherwise
+ */
+int sdma_set_channel_priority(struct sdma_channel *sdmac,
+		unsigned int priority);
+
+/**
+ * sdma_setup_channel() - convenience function for setting channel ownership
+ * @sdmac:	pointer to sdma_channel object
+ * @external:	if true, script is triggered by an external event,
+ *		if false, script is triggered by the CPU
+ */
+void sdma_setup_channel(struct sdma_channel *sdmac, bool external);
+
+/**
+ * sdma_event_enable() - allows a channel to be triggered by the numbered event
+ * @sdmac:	pointer to sdma_channel object
+ * @event:	event number (see reference manual)
+ */
+void sdma_event_enable(struct sdma_channel *sdmac, unsigned int event);
+
+/**
+ * sdma_event_disable() - prevents a channel from being triggered by an event
+ * @sdmac:	pointer to sdma_channel object
+ * @event:	event number (see reference manual)
+ */
+void sdma_event_disable(struct sdma_channel *sdmac, unsigned int event);
+
+/* address should be in program space (halfword addressing) */
+
+/**
+ * sdma_load_script() - copies script from ARM memory to SDMA memory
+ * @sdma:	pointer to sdma_engine object
+ * @buf:	start of script
+ * @size:	size of script in bytes
+ * @address:	destination address in SDMA program space
+ *		(using halfword addressing)
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
+		u32 address);
+
+/**
+ * sdma_load_partial_context() - writes a subset of a channel's context
+ * @sdmac:		pointer to sdma_channel object
+ * @context:		pointer to data to write
+ * @byte_offset:	destination offset within the channel's context RAM
+ *			(must be a multiple of 4 and less than 128)
+ * @num_bytes:		number of bytes to copy into the channel's context RAM
+ *			(must be > 0 and <= 128)
+ *
+ * Can be used to update a subset of a channel's registers while leaving others
+ * undisturbed, e.g. to change a script's arguments while it is running without
+ * overwriting internal state.
+ * Since RAM loading is handled by channel 0, and channels cannot preempt each
+ * other, the load operation is mutually exclusive with the channel's execution.
+ * (i.e. a channel's registers will not change while its script is executing.)
+ *
+ * Example: to update a channel's entire context, use byte_offset=0 and
+ * num_bytes=128.
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_load_partial_context(struct sdma_channel *sdmac,
+	struct sdma_context_data *context,
+	u32 byte_offset,
+	u32 num_bytes);
+
+/* size should be a value in bytes */
+/* address should be in data space (word addressing) */
+
+/**
+ * sdma_write_datamem() - writes data into the SDMA engine's address space
+ * @sdma:	pointer to sdma_engine object
+ * @buf:	data to write
+ * @size:	number of bytes to write
+ * @address:	destination offset, in 32-bit words, from the origin of SDMA
+ *		address space
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_write_datamem(struct sdma_engine *sdma, void *buf, int size,
+	u32 address);
+
+/**
+ * sdma_fetch_partial_context() - reads a subset of a channel's context
+ * @sdmac:		pointer to sdma_channel object
+ * @buf:		buffer to receive data
+ * @byte_offset:	source offset within the channel's context RAM
+ *			(must be a multiple of 4 and less than 128)
+ * @num_bytes:		number of bytes to read from the channel's context RAM
+ *			(must be > 0 and <= 128)
+ *
+ * Since RAM loading is handled by channel 0, and channels cannot preempt each
+ * other, the fetch operation is mutually exclusive with the channel's
+ * execution. (i.e. the values will not be changing at the same time as they are
+ * being read.)
+ *
+ * Example: to fetch a channel's entire context, use byte_offset=0 and
+ * num_bytes=128.
+ *
+ * buf must be large enough to hold num_bytes of data.
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_fetch_partial_context(struct sdma_channel *sdmac, void *buf,
+    u32 byte_offset,
+    u32 num_bytes);
+
+/**
+ * sdma_print_context() - dump string representation of channel context values
+ * @sdma:	pointer to sdma_engine object
+ * @channel:	channel number, 0-31
+ * @buf:	buffer to receive the string
+ *
+ * Prints a string representation of all channel registers and scratch memory
+ * words. buf should be at least 512 bytes long. Useful for debugging.
+ *
+ * Return: result string length in bytes, or < 0 on error
+ */
+ssize_t sdma_print_context(struct sdma_engine *sdma, int channel, char *buf);
+
 #endif /* __MACH_MXC_SDMA_H__ */
diff -u a/drivers\dma\imx-sdma.c b/drivers\dma\imx-sdma.c
--- a/drivers/dma/imx-sdma.c
+++ b/drivers/dma/imx-sdma.c
@@ -3,12 +3,11 @@
  *
  * This file contains a driver for the Freescale Smart DMA engine
  *
- * Copyright 2010 Sascha Hauer, Pengutronix <s.hauer@pengutronix.de>
  *
- * Based on code from Freescale:
- *
- * Copyright 2004-2016 Freescale Semiconductor, Inc. All Rights Reserved.
- * Copyright 2018 NXP.
+ * Copyright (C) 2018 Scott Wiederhold <s.e.wiederhold@gmail.com>
+ * Portions Copyright (C) 2015-2018 Glowforge, Inc. <opensource@glowforge.com>
+ * Portions Copyright 2004-2016 Freescale Semiconductor, Inc. All Rights Reserved.
+ * Portions Copyright 2010 Sascha Hauer, Pengutronix <s.hauer@pengutronix.de>
  *
  * The code contained herein is licensed under the GNU General Public
  * License. You may obtain a copy of the GNU General Public License
@@ -33,6 +32,7 @@
 #include <linux/device.h>
 #include <linux/genalloc.h>
 #include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
 #include <linux/firmware.h>
 #include <linux/slab.h>
 #include <linux/platform_device.h>
@@ -41,7 +41,6 @@
 #include <linux/of_address.h>
 #include <linux/of_device.h>
 #include <linux/of_dma.h>
-#include <linux/workqueue.h>

 #include <asm/irq.h>
 #include <linux/platform_data/dma-imx-sdma.h>
@@ -83,9 +82,6 @@
 #define SDMA_CHNENBL0_IMX35	0x200
 #define SDMA_CHNENBL0_IMX31	0x080
 #define SDMA_CHNPRI_0		0x100
-#define SDMA_DONE0_CONFIG	0x1000
-#define SDMA_DONE0_CONFIG_DONE_SEL	0x7
-#define SDMA_DONE0_CONFIG_DONE_DIS	0x6

 /*
  * Buffer descriptor status values.
@@ -187,129 +183,6 @@
 #define SDMA_WATERMARK_LEVEL_HWE	BIT(29)
 #define SDMA_WATERMARK_LEVEL_CONT	BIT(31)

-#define SDMA_DMA_BUSWIDTHS	(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \
-				 BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \
-				 BIT(DMA_SLAVE_BUSWIDTH_3_BYTES) | \
-				 BIT(DMA_SLAVE_BUSWIDTH_4_BYTES))
-
-#define SDMA_DMA_DIRECTIONS	(BIT(DMA_DEV_TO_MEM) | \
-				 BIT(DMA_MEM_TO_DEV) | \
-				 BIT(DMA_DEV_TO_DEV))
-
-#define SDMA_WATERMARK_LEVEL_FIFOS_OFF	12
-#define SDMA_WATERMARK_LEVEL_SW_DONE	BIT(23)
-#define SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF 24
-
-/*
- * Mode/Count of data node descriptors - IPCv2
- */
-struct sdma_mode_count {
-	u32 count   : 16; /* size of the buffer pointed by this BD */
-	u32 status  :  8; /* E,R,I,C,W,D status bits stored here */
-	u32 command :  8; /* command mostly used for channel 0 */
-};
-
-/*
- * Buffer descriptor
- */
-struct sdma_buffer_descriptor {
-	struct sdma_mode_count  mode;
-	u32 buffer_addr;	/* address of the buffer described */
-	u32 ext_buffer_addr;	/* extended buffer address */
-} __attribute__ ((packed));
-
-/**
- * struct sdma_channel_control - Channel control Block
- *
- * @current_bd_ptr	current buffer descriptor processed
- * @base_bd_ptr		first element of buffer descriptor array
- * @unused		padding. The SDMA engine expects an array of 128 byte
- *			control blocks
- */
-struct sdma_channel_control {
-	u32 current_bd_ptr;
-	u32 base_bd_ptr;
-	u32 unused[2];
-} __attribute__ ((packed));
-
-/**
- * struct sdma_state_registers - SDMA context for a channel
- *
- * @pc:		program counter
- * @t:		test bit: status of arithmetic & test instruction
- * @rpc:	return program counter
- * @sf:		source fault while loading data
- * @spc:	loop start program counter
- * @df:		destination fault while storing data
- * @epc:	loop end program counter
- * @lm:		loop mode
- */
-struct sdma_state_registers {
-	u32 pc     :14;
-	u32 unused1: 1;
-	u32 t      : 1;
-	u32 rpc    :14;
-	u32 unused0: 1;
-	u32 sf     : 1;
-	u32 spc    :14;
-	u32 unused2: 1;
-	u32 df     : 1;
-	u32 epc    :14;
-	u32 lm     : 2;
-} __attribute__ ((packed));
-
-/**
- * struct sdma_context_data - sdma context specific to a channel
- *
- * @channel_state:	channel state bits
- * @gReg:		general registers
- * @mda:		burst dma destination address register
- * @msa:		burst dma source address register
- * @ms:			burst dma status register
- * @md:			burst dma data register
- * @pda:		peripheral dma destination address register
- * @psa:		peripheral dma source address register
- * @ps:			peripheral dma status register
- * @pd:			peripheral dma data register
- * @ca:			CRC polynomial register
- * @cs:			CRC accumulator register
- * @dda:		dedicated core destination address register
- * @dsa:		dedicated core source address register
- * @ds:			dedicated core status register
- * @dd:			dedicated core data register
- */
-struct sdma_context_data {
-	struct sdma_state_registers  channel_state;
-	u32  gReg[8];
-	u32  mda;
-	u32  msa;
-	u32  ms;
-	u32  md;
-	u32  pda;
-	u32  psa;
-	u32  ps;
-	u32  pd;
-	u32  ca;
-	u32  cs;
-	u32  dda;
-	u32  dsa;
-	u32  ds;
-	u32  dd;
-	u32  scratch0;
-	u32  scratch1;
-	u32  scratch2;
-	u32  scratch3;
-	u32  scratch4;
-	u32  scratch5;
-	u32  scratch6;
-	u32  scratch7;
-} __attribute__ ((packed));
-
-#define NUM_BD (int)(PAGE_SIZE / sizeof(struct sdma_buffer_descriptor))
-#define SDMA_BD_MAX_CNT	0xfffc /* align with 4 bytes */
-
-struct sdma_engine;
-
 struct sdma_desc {
 	struct virt_dma_desc		vd;
 	struct list_head		node;
@@ -343,7 +216,7 @@
 	struct sdma_engine		*sdma;
 	struct sdma_desc		*desc;
 	unsigned int			channel;
-	enum dma_transfer_direction		direction;
+	enum dma_transfer_direction	direction;
 	enum sdma_peripheral_type	peripheral_type;
 	unsigned int			event_id0;
 	unsigned int			event_id1;
@@ -365,13 +238,13 @@
 	u32				bd_size_sum;
 	bool				src_dualfifo;
 	bool				dst_dualfifo;
-	unsigned int			fifo_num;
-	bool				sw_done;
-	u32				sw_done_sel;
-	struct work_struct              terminate_worker;
+	struct dma_pool			*bd_pool;
+	struct dma_async_tx_descriptor 	cb;
+	struct tasklet_struct		cb_task;
 };

 #define IMX_DMA_SG_LOOP		BIT(0)
+#define IMX_DMA_CUSTOM_CALLBACK	BIT(1)

 #define MAX_DMA_CHANNELS 32
 #define MXC_SDMA_DEFAULT_PRIORITY 1
@@ -439,15 +312,23 @@
 	u32				spba_end_addr;
 	unsigned int			irq;
 	struct gen_pool 		*iram_pool;
+	void				*tiny_datamem_buf;
+	dma_addr_t			tiny_datamem_buf_phys;
 	/* channel0 bd */
 	dma_addr_t			bd0_phys;
 	bool				bd0_iram;
 	struct sdma_buffer_descriptor	*bd0;
-	bool				fw_loaded;
-	int				idx;
-	/* clock ration for AHB:SDMA core. 1:1 is 1, 2:1 is 0*/
-	bool				clk_ratio;
+	bool				suspend_off;
 };
+
+/**
+ * If nonzero, sdma_write_datamem()/sdma_fetch_datamem()
+ * use a single preallocated temporary buffer for transfers whose size is
+ * less than or equal to this value.
+ */
+#define TINY_DATAMEM_BUF_SIZE	PAGE_SIZE
+
+static struct sdma_engine *sdma_singleton = NULL;

 static struct sdma_driver_data sdma_imx31 = {
 	.chnenbl0 = SDMA_CHNENBL0_IMX31,
@@ -575,12 +456,6 @@
 };

 static struct sdma_driver_data sdma_imx7d = {
-	.chnenbl0 = SDMA_CHNENBL0_IMX35,
-	.num_events = 48,
-	.script_addrs = &sdma_script_imx7d,
-};
-
-static struct sdma_driver_data sdma_imx8m = {
 	.chnenbl0 = SDMA_CHNENBL0_IMX35,
 	.num_events = 48,
 	.script_addrs = &sdma_script_imx7d,
@@ -612,9 +487,6 @@
 		.name = "imx7d-sdma",
 		.driver_data = (unsigned long)&sdma_imx7d,
 	}, {
-		.name = "imx8mq-sdma",
-		.driver_data = (unsigned long)&sdma_imx8m,
-	}, {
 		/* sentinel */
 	}
 };
@@ -630,18 +502,22 @@
 	{ .compatible = "fsl,imx31-sdma", .data = &sdma_imx31, },
 	{ .compatible = "fsl,imx25-sdma", .data = &sdma_imx25, },
 	{ .compatible = "fsl,imx7d-sdma", .data = &sdma_imx7d, },
-	{ .compatible = "fsl,imx8mq-sdma", .data = &sdma_imx8m, },
 	{ /* sentinel */ }
 };
 MODULE_DEVICE_TABLE(of, sdma_dt_ids);
-
-static int sdma_dev_idx;

 #define SDMA_H_CONFIG_DSPDMA	BIT(12) /* indicates if the DSPDMA is used */
 #define SDMA_H_CONFIG_RTD_PINS	BIT(11) /* indicates if Real-Time Debug pins are enabled */
 #define SDMA_H_CONFIG_ACR	BIT(4)  /* indicates if AHB freq /core freq = 2 or 1 */
 #define SDMA_H_CONFIG_CSM	(3)       /* indicates which context switch mode is selected*/

+/* returns an address in data space (32-bit words) */
+u32 sdma_channel_context_base(int ch)
+{
+	return 2048 + (sizeof(struct sdma_context_data) / 4) * ch;
+}
+EXPORT_SYMBOL(sdma_channel_context_base);
+
 static void sdma_start_desc(struct sdma_channel *sdmac);

 static inline u32 chnenbl_ofs(struct sdma_engine *sdma, unsigned int event)
@@ -650,7 +526,7 @@
 	return chnenbl0 + event * 4;
 }

-static int sdma_config_ownership(struct sdma_channel *sdmac,
+int sdma_config_ownership(struct sdma_channel *sdmac,
 		bool event_override, bool mcu_override, bool dsp_override)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
@@ -685,6 +561,7 @@

 	return 0;
 }
+EXPORT_SYMBOL(sdma_config_ownership);

 static void sdma_enable_channel(struct sdma_engine *sdma, int channel)
 {
@@ -713,7 +590,7 @@
 	return ret;
 }

-static int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
+int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
 		u32 address)
 {
 	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
@@ -752,8 +629,126 @@

 	return ret;
 }
-
-static void sdma_event_enable(struct sdma_channel *sdmac, unsigned int event)
+EXPORT_SYMBOL(sdma_load_script);
+
+int sdma_write_datamem(struct sdma_engine *sdma, void *buf, int size,
+		u32 address)
+{
+	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	void *buf_virt;
+	dma_addr_t buf_phys;
+	int ret;
+	unsigned long flags;
+	bool use_iram = true;
+	bool tiny = (size <= TINY_DATAMEM_BUF_SIZE);
+
+	if (!tiny) {
+		buf_virt = gen_pool_dma_alloc(sdma->iram_pool, size, &buf_phys);
+		if (!buf_virt) {
+			use_iram = false;
+			buf_virt = dma_alloc_coherent(NULL, size, &buf_phys,
+					GFP_KERNEL);
+			if (!buf_virt)
+				return -ENOMEM;
+		}
+	} else {
+		buf_virt = sdma->tiny_datamem_buf;
+		buf_phys = sdma->tiny_datamem_buf_phys;
+	}
+
+	spin_lock_irqsave(&sdma->channel_0_lock, flags);
+
+	bd0->mode.command = C0_SETDM;
+	bd0->mode.status = BD_DONE | BD_INTR | BD_WRAP | BD_EXTD;
+	bd0->mode.count = size / 4;
+	bd0->buffer_addr = buf_phys;
+	bd0->ext_buffer_addr = address;
+
+	memcpy(buf_virt, buf, size);
+
+	ret = sdma_run_channel0(sdma);
+
+	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
+
+	if (!tiny) {
+		if (use_iram)
+			gen_pool_free(sdma->iram_pool, (unsigned long)buf_virt,
+					size);
+		else
+			dma_free_coherent(NULL, size, buf_virt, buf_phys);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(sdma_write_datamem);
+
+int sdma_fetch_datamem(struct sdma_engine *sdma, void *buf, int size,
+		u32 address)
+{
+	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	void *buf_virt;
+	dma_addr_t buf_phys;
+	int ret;
+	unsigned long flags;
+	bool use_iram = true;
+	bool tiny = (size <= TINY_DATAMEM_BUF_SIZE);
+
+	if (!tiny) {
+		buf_virt = gen_pool_dma_alloc(sdma->iram_pool, size, &buf_phys);
+		if (!buf_virt) {
+			use_iram = false;
+			buf_virt = dma_alloc_coherent(NULL, size, &buf_phys,
+					GFP_KERNEL);
+			if (!buf_virt)
+				return -ENOMEM;
+		}
+	} else {
+		buf_virt = sdma->tiny_datamem_buf;
+		buf_phys = sdma->tiny_datamem_buf_phys;
+	}
+	spin_lock_irqsave(&sdma->channel_0_lock, flags);
+
+	bd0->mode.command = C0_GETDM;
+	bd0->mode.status = BD_DONE | BD_INTR | BD_WRAP | BD_EXTD;
+	bd0->mode.count = size / 4;
+	bd0->buffer_addr = buf_phys;
+	bd0->ext_buffer_addr = address;
+
+	ret = sdma_run_channel0(sdma);
+
+	memcpy(buf, buf_virt, size);
+
+	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
+
+	if (!tiny) {
+		if (use_iram)
+			gen_pool_free(sdma->iram_pool, (unsigned long)buf_virt,
+					size);
+		else
+			dma_free_coherent(NULL, size, buf_virt, buf_phys);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(sdma_fetch_datamem);
+
+int sdma_fetch_partial_context(struct sdma_channel *sdmac, void *buf,
+	u32 byte_offset, u32 num_bytes)
+{
+	static const u32 csz = sizeof(struct sdma_context_data);
+	u32 addr;
+	if (num_bytes > csz || num_bytes == 0 ||
+	byte_offset >= csz || byte_offset+num_bytes > csz ||
+	num_bytes % sizeof(u32) || byte_offset % sizeof(u32)) {
+		dev_err(sdmac->sdma->dev, "%s: invalid offset/length", __func__);
+		return -EINVAL;
+	}
+	addr = sdma_channel_context_base(sdmac->channel) + byte_offset/sizeof(u32);
+	return sdma_fetch_datamem(sdmac->sdma, buf, num_bytes, addr);
+}
+EXPORT_SYMBOL(sdma_fetch_partial_context);
+
+void sdma_event_enable(struct sdma_channel *sdmac, unsigned int event)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
 	int channel = sdmac->channel;
@@ -763,24 +758,10 @@
 	val = readl_relaxed(sdma->regs + chnenbl);
 	__set_bit(channel, &val);
 	writel_relaxed(val, sdma->regs + chnenbl);
-
-	/* Set SDMA_DONEx_CONFIG is sw_done enabled */
-	if (sdmac->sw_done) {
-		u32 offset = SDMA_DONE0_CONFIG + sdmac->sw_done_sel / 4;
-		u32 done_sel = SDMA_DONE0_CONFIG_DONE_SEL +
-				((sdmac->sw_done_sel % 4) << 3);
-		u32 sw_done_dis = SDMA_DONE0_CONFIG_DONE_DIS +
-				((sdmac->sw_done_sel % 4) << 3);
-
-		val = readl_relaxed(sdma->regs + offset);
-		__set_bit(done_sel, &val);
-		__clear_bit(sw_done_dis, &val);
-		writel_relaxed(val, sdma->regs + offset);
-	}
-
-}
-
-static void sdma_event_disable(struct sdma_channel *sdmac, unsigned int event)
+}
+EXPORT_SYMBOL(sdma_event_enable);
+
+void sdma_event_disable(struct sdma_channel *sdmac, unsigned int event)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
 	int channel = sdmac->channel;
@@ -791,10 +772,12 @@
 	__clear_bit(channel, &val);
 	writel_relaxed(val, sdma->regs + chnenbl);
 }
+EXPORT_SYMBOL(sdma_event_disable);

 static void sdma_update_channel_loop(struct sdma_channel *sdmac)
 {
 	struct sdma_buffer_descriptor *bd;
+	struct sdma_desc *desc = sdmac->desc;
 	int error = 0;
 	enum dma_status	old_status = sdmac->status;

@@ -802,9 +785,7 @@
 	 * loop mode. Iterate over descriptors, re-setup them and
 	 * call callback function.
 	 */
-	while (sdmac->desc) {
-		struct sdma_desc *desc = sdmac->desc;
-
+	while (desc) {
 		bd = &desc->bd[desc->buf_tail];

 		if (bd->mode.status & BD_DONE)
@@ -865,13 +846,18 @@
 		sdmac->status = DMA_COMPLETE;
 }

+static void sdma_custom_callback_tasklet(unsigned long data)
+{
+	struct sdma_channel *sdmac = (struct sdma_channel *) data;
+
+	if (sdmac->cb.callback)
+		sdmac->cb.callback(sdmac->cb.callback_param);
+}
+
 static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 {
 	struct sdma_engine *sdma = dev_id;
-	unsigned long stat;
-
-	clk_enable(sdma->clk_ipg);
-	clk_enable(sdma->clk_ahb);
+	unsigned long stat, flags;

 	stat = readl_relaxed(sdma->regs + SDMA_H_INTR);
 	writel_relaxed(stat, sdma->regs + SDMA_H_INTR);
@@ -883,9 +869,11 @@
 		struct sdma_channel *sdmac = &sdma->channel[channel];
 		struct sdma_desc *desc;

-		spin_lock(&sdmac->vc.lock);
+		spin_lock_irqsave(&sdmac->vc.lock, flags);
 		desc = sdmac->desc;
-		if (desc) {
+		if (sdmac->flags & IMX_DMA_CUSTOM_CALLBACK && sdmac->cb.callback) {
+			tasklet_schedule(&sdmac->cb_task);
+		} else if (desc) {
 			if (sdmac->flags & IMX_DMA_SG_LOOP) {
 				if (sdmac->peripheral_type != IMX_DMATYPE_HDMI)
 					sdma_update_channel_loop(sdmac);
@@ -900,11 +888,8 @@
 			}
 		}
 		__clear_bit(channel, &stat);
-		spin_unlock(&sdmac->vc.lock);
-	}
-
-	clk_disable(sdma->clk_ipg);
-	clk_disable(sdma->clk_ahb);
+		spin_unlock_irqrestore(&sdmac->vc.lock, flags);
+	}

 	return IRQ_HANDLED;
 }
@@ -1002,9 +987,6 @@
 	case IMX_DMATYPE_HDMI:
 		emi_2_per = sdma->script_addrs->hdmi_dma_addr;
 		break;
-	case IMX_DMATYPE_MULTI_SAI:
-		per_2_emi = sdma->script_addrs->sai_2_mcu_addr;
-		emi_2_per = sdma->script_addrs->mcu_2_sai_addr;
 	default:
 		break;
 	}
@@ -1070,7 +1052,7 @@
 	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
 	bd0->mode.count = sizeof(*context) / 4;
 	bd0->buffer_addr = sdma->context_phys;
-	bd0->ext_buffer_addr = 2048 + (sizeof(*context) / 4) * channel;
+	bd0->ext_buffer_addr = sdma_channel_context_base(channel);
 	ret = sdma_run_channel0(sdma);

 	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
@@ -1080,14 +1062,54 @@
 	return ret;
 }

+static int sdma_save_restore_context(struct sdma_engine *sdma, bool save)
+{
+	struct sdma_context_data *context = sdma->context;
+	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	unsigned long flags;
+	int ret;
+
+	spin_lock_irqsave(&sdma->channel_0_lock, flags);
+
+	if (save)
+		bd0->mode.command = C0_GETDM;
+	else
+		bd0->mode.command = C0_SETDM;
+
+	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
+	bd0->mode.count = MAX_DMA_CHANNELS * sizeof(*context) / 4;
+	bd0->buffer_addr = sdma->context_phys;
+	bd0->ext_buffer_addr = 2048;
+	ret = sdma_run_channel0(sdma);
+
+	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
+
+	return ret;
+}
+
 static struct sdma_channel *to_sdma_chan(struct dma_chan *chan)
 {
 	return container_of(chan, struct sdma_channel, vc.chan);
 }

-static int sdma_disable_channel(struct dma_chan *chan)
-{
-	struct sdma_channel *sdmac = to_sdma_chan(chan);
+int sdma_load_partial_context(struct sdma_channel *sdmac,
+	struct sdma_context_data *context, u32 byte_offset, u32 num_bytes)
+{
+	static const u32 csz = sizeof(*context);
+	u32 addr;
+	if (num_bytes > csz || num_bytes == 0 ||
+	byte_offset >= csz || byte_offset+num_bytes > csz ||
+	num_bytes % sizeof(u32) || byte_offset % sizeof(u32)) {
+		dev_err(sdmac->sdma->dev, "%s: invalid offset/length", __func__);
+		return -EINVAL;
+	}
+	addr = sdma_channel_context_base(sdmac->channel) + byte_offset/sizeof(u32);
+	return sdma_write_datamem(sdmac->sdma, context, num_bytes, addr);
+}
+EXPORT_SYMBOL(sdma_load_partial_context);
+
+static int sdma_disable_channel(struct sdma_channel *sdmac)
+{
 	struct sdma_engine *sdma = sdmac->sdma;
 	int channel = sdmac->channel;

@@ -1142,32 +1164,12 @@
 		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_DD;
 }

-static void sdma_set_watermarklevel_for_sais(struct sdma_channel *sdmac)
-{
-	sdmac->watermark_level &= ~(0xFF << SDMA_WATERMARK_LEVEL_FIFOS_OFF |
-				    SDMA_WATERMARK_LEVEL_SW_DONE |
-				    0xf << SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF);
-
-	if (sdmac->sw_done)
-		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_SW_DONE |
-			sdmac->sw_done_sel <<
-			SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF;
-
-	/* For fifo_num
-	 * bit 12-15 is the fifo number;
-	 * bit 16-19 is the fifo offset,
-	 * so here only need to shift left fifo_num 12 bit for watermake_level
-	 */
-	sdmac->watermark_level |= sdmac->fifo_num<<
-				SDMA_WATERMARK_LEVEL_FIFOS_OFF;
-}
-
 static int sdma_config_channel(struct dma_chan *chan)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	int ret;

-	sdma_disable_channel(chan);
+	sdma_disable_channel(sdmac);

 	sdmac->event_mask[0] = 0;
 	sdmac->event_mask[1] = 0;
@@ -1211,9 +1213,6 @@
 			    sdmac->direction == DMA_MEM_TO_DEV &&
 			    sdmac->sdma->drvdata == &sdma_imx6ul)
 				__set_bit(31, &sdmac->watermark_level);
-			else if (sdmac->peripheral_type ==
-					IMX_DMATYPE_MULTI_SAI)
-				sdma_set_watermarklevel_for_sais(sdmac);

 			__set_bit(sdmac->event_id0, sdmac->event_mask);
 		}
@@ -1225,14 +1224,12 @@
 		sdmac->watermark_level = 0; /* FIXME: M3_BASE_ADDRESS */
 	}

-	sdmac->context_loaded = false;
-
 	ret = sdma_load_context(sdmac);

 	return ret;
 }

-static int sdma_set_channel_priority(struct sdma_channel *sdmac,
+int sdma_set_channel_priority(struct sdma_channel *sdmac,
 		unsigned int priority)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
@@ -1247,6 +1244,7 @@

 	return 0;
 }
+EXPORT_SYMBOL(sdma_set_channel_priority);

 static int sdma_alloc_bd(struct sdma_desc *desc)
 {
@@ -1259,14 +1257,16 @@
 				      &desc->bd_phys);
 	if (!desc->bd) {
 		desc->bd_iram = false;
-		desc->bd = dma_zalloc_coherent(desc->sdmac->sdma->dev, bd_size,
-				&desc->bd_phys, GFP_NOWAIT);
+		desc->bd = dma_pool_alloc(desc->sdmac->bd_pool, GFP_ATOMIC,
+						&desc->bd_phys);
 		if (!desc->bd)
 			return ret;
 	}
 	spin_lock_irqsave(&desc->sdmac->vc.lock, flags);
 	desc->sdmac->bd_size_sum += bd_size;
 	spin_unlock_irqrestore(&desc->sdmac->vc.lock, flags);
+
+	memset(desc->bd, 0, bd_size);

 	return 0;
 }
@@ -1281,8 +1281,8 @@
 			gen_pool_free(desc->sdmac->sdma->iram_pool,
 				     (unsigned long)desc->bd, bd_size);
 		else
-			dma_free_coherent(desc->sdmac->sdma->dev, bd_size,
-					desc->bd, desc->bd_phys);
+			dma_pool_free(desc->sdmac->bd_pool, desc->bd,
+					desc->bd_phys);
 		spin_lock_irqsave(&desc->sdmac->vc.lock, flags);
 		desc->sdmac->bd_size_sum -= bd_size;
 		spin_unlock_irqrestore(&desc->sdmac->vc.lock, flags);
@@ -1317,6 +1317,23 @@
 	return ret;
 }

+void sdma_set_channel_interrupt_callback(struct sdma_channel *sdmac,
+		dma_async_tx_callback int_cb, void *cb_param)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&sdmac->vc.lock, flags);
+	if (int_cb)
+		sdmac->flags |= IMX_DMA_CUSTOM_CALLBACK;
+	else
+		sdmac->flags &= ~IMX_DMA_CUSTOM_CALLBACK;
+	sdmac->cb.callback = int_cb;
+	sdmac->cb.callback_param = cb_param;
+	tasklet_init(&sdmac->cb_task, sdma_custom_callback_tasklet,
+					 (unsigned long) sdmac);
+	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
+}
+EXPORT_SYMBOL(sdma_set_channel_interrupt_callback);
+
 static struct sdma_desc *to_sdma_desc(struct dma_async_tx_descriptor *t)
 {
 	return container_of(t, struct sdma_desc, vd.tx);
@@ -1339,7 +1356,7 @@
 	if (!(sdmac->flags & IMX_DMA_SG_LOOP))
 		return -EINVAL;

-	sdma_disable_channel(chan);
+	sdma_disable_channel(sdmac);
 	spin_lock_irqsave(&sdmac->vc.lock, flags);
 	sdmac->status = DMA_PAUSED;
 	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
@@ -1350,10 +1367,21 @@
 static int sdma_channel_resume(struct dma_chan *chan)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
+	struct sdma_engine *sdma = sdmac->sdma;
 	unsigned long flags;

 	if (!(sdmac->flags & IMX_DMA_SG_LOOP))
 		return -EINVAL;
+
+	/*
+	 * restore back context since context may loss if mega/fast OFF
+	 */
+	if (sdma->suspend_off) {
+		if (sdma_load_context(sdmac)) {
+			dev_err(sdmac->sdma->dev, "context load failed.\n");
+			return -EINVAL;
+		}
+	}

 	sdma_enable_channel(sdmac->sdma, sdmac->channel);
 	spin_lock_irqsave(&sdmac->vc.lock, flags);
@@ -1363,10 +1391,9 @@
 	return 0;
 }

-static void sdma_channel_terminate_work(struct work_struct *work)
-{
-	struct sdma_channel *sdmac = container_of(work, struct sdma_channel,
-						  terminate_worker);
+static int sdma_terminate_all(struct dma_chan *chan)
+{
+	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	unsigned long flags;
 	LIST_HEAD(head);

@@ -1380,36 +1407,15 @@
 		spin_unlock_irqrestore(&sdmac->vc.lock, flags);
 		sdmac->vc.desc_free(&desc->vd);
 		spin_lock_irqsave(&sdmac->vc.lock, flags);
-		sdmac->vc.cyclic = NULL;
 	}
 	if (sdmac->desc)
 		sdmac->desc = NULL;
 	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
 	vchan_dma_desc_free_list(&sdmac->vc, &head);
-
+	sdma_disable_channel(sdmac);
 	sdmac->context_loaded = false;

-}
-
-static int sdma_terminate_all(struct dma_chan *chan)
-{
-	struct sdma_channel *sdmac = to_sdma_chan(chan);
-
-	sdma_disable_channel(chan);
-
-	if (sdmac->desc)
-		schedule_work(&sdmac->terminate_worker);
-
 	return 0;
-}
-
-static void sdma_wait_tasklet(struct dma_chan *chan)
-{
-	struct sdma_channel *sdmac = to_sdma_chan(chan);
-
-	tasklet_kill(&sdmac->vc.task);
-
-	flush_work(&sdmac->terminate_worker);
 }

 static int sdma_alloc_chan_resources(struct dma_chan *chan)
@@ -1461,17 +1467,16 @@
 	sdmac->event_id1 = data->dma_request2;
 	sdmac->src_dualfifo = data->src_dualfifo;
 	sdmac->dst_dualfifo = data->dst_dualfifo;
-	/* Get software done selector if sw_done enabled */
-	if (data->done_sel & BIT(31)) {
-		sdmac->sw_done = true;
-		sdmac->sw_done_sel = (data->done_sel >> 8) & 0xff;
-	}

 	ret = sdma_set_channel_priority(sdmac, prio);
 	if (ret)
 		goto disable_clk_ahb;

 	sdmac->bd_size_sum = 0;
+
+	sdmac->bd_pool = dma_pool_create("bd_pool", chan->device->dev,
+				sizeof(struct sdma_buffer_descriptor),
+				32, 0);

 	return 0;

@@ -1489,8 +1494,6 @@

 	sdma_terminate_all(chan);

-	sdma_wait_tasklet(chan);
-
 	sdma_event_disable(sdmac, sdmac->event_id0);
 	if (sdmac->event_id1)
 		sdma_event_disable(sdmac, sdmac->event_id1);
@@ -1502,18 +1505,15 @@

 	clk_disable(sdma->clk_ipg);
 	clk_disable(sdma->clk_ahb);
+
+	dma_pool_destroy(sdmac->bd_pool);
+	sdmac->bd_pool = NULL;
 }

 static struct sdma_desc *sdma_transfer_init(struct sdma_channel *sdmac,
 			      enum dma_transfer_direction direction, u32 bds)
 {
 	struct sdma_desc *desc;
-
-	if (!sdmac->sdma->fw_loaded) {
-		dev_err(sdmac->sdma->dev, "sdma firmware not ready!\n");
-		goto err_out;
-	}
-
 	/* Now allocate and setup the descriptor. */
 	desc = kzalloc((sizeof(*desc)), GFP_ATOMIC);
 	if (!desc)
@@ -1554,9 +1554,6 @@
 		bd->mode.command = 0;
 		if ((count | dma_dst | dma_src) & 3)
 			ret = -EINVAL;
-		break;
-	case DMA_SLAVE_BUSWIDTH_3_BYTES:
-		bd->mode.command = 3;
 		break;
 	case DMA_SLAVE_BUSWIDTH_2_BYTES:
 		bd->mode.command = 2;
@@ -1619,7 +1616,7 @@
 			param &= ~BD_CONT;
 		}

-		dev_dbg(sdma->dev, "entry %d: count: %zd dma: 0x%x %s%s\n",
+		dev_dbg(sdma->dev, "entry %d: count: %d dma: 0x%u %s%s\n",
 				i, count, bd->buffer_addr,
 				param & BD_WRAP ? "wrap" : "",
 				param & BD_INTR ? " intr" : "");
@@ -1792,7 +1789,7 @@
 		if (i + 1 == num_periods)
 			param |= BD_WRAP;

-		dev_dbg(sdma->dev, "entry %d: count: %zu dma: %pad %s%s\n",
+		dev_dbg(sdma->dev, "entry %d: count: %d dma: %pad %s%s\n",
 				i, period_len, &dma_addr,
 				param & BD_WRAP ? "wrap" : "",
 				param & BD_INTR ? " intr" : "");
@@ -1817,14 +1814,12 @@
 		       struct dma_slave_config *dmaengine_cfg)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
-	/* clear watermark_level before setting */
-	sdmac->watermark_level = 0;
+
 	if (dmaengine_cfg->direction == DMA_DEV_TO_MEM) {
 		sdmac->per_address = dmaengine_cfg->src_addr;
 		sdmac->watermark_level = dmaengine_cfg->src_maxburst *
 			dmaengine_cfg->src_addr_width;
 		sdmac->word_size = dmaengine_cfg->src_addr_width;
-		sdmac->fifo_num =  dmaengine_cfg->src_fifo_num;
 	} else if (dmaengine_cfg->direction == DMA_DEV_TO_DEV) {
 		sdmac->per_address2 = dmaengine_cfg->src_addr;
 		sdmac->per_address = dmaengine_cfg->dst_addr;
@@ -1844,10 +1839,16 @@
 		sdmac->watermark_level = dmaengine_cfg->dst_maxburst *
 			dmaengine_cfg->dst_addr_width;
 		sdmac->word_size = dmaengine_cfg->dst_addr_width;
-		sdmac->fifo_num =  dmaengine_cfg->dst_fifo_num;
 	}
 	sdmac->direction = dmaengine_cfg->direction;
 	return sdma_config_channel(chan);
+}
+
+static void sdma_wait_tasklet(struct dma_chan *chan)
+{
+	struct sdma_channel *sdmac = to_sdma_chan(chan);
+
+	tasklet_kill(&sdmac->vc.task);
 }

 static enum dma_status sdma_tx_status(struct dma_chan *chan,
@@ -1933,8 +1934,8 @@

 #define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1	34
 #define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V2	38
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V3	43
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V4	44
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V3	41
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V4	42

 static void sdma_add_scripts(struct sdma_engine *sdma,
 		const struct sdma_script_start_addrs *addr)
@@ -1965,7 +1966,7 @@
 		return;
 	}

-	if (fw->size < sizeof(*header) || sdma->fw_loaded)
+	if (fw->size < sizeof(*header))
 		goto err_firmware;

 	header = (struct sdma_firmware_header *)fw->data;
@@ -2009,8 +2010,6 @@
 	dev_info(sdma->dev, "loaded firmware %d.%d\n",
 			header->version_major,
 			header->version_minor);
-
-	sdma->fw_loaded = true;

 err_firmware:
 	release_firmware(fw);
@@ -2122,6 +2121,20 @@
 		}
 	}

+#if TINY_DATAMEM_BUF_SIZE
+	sdma->tiny_datamem_buf = gen_pool_dma_alloc(sdma->iram_pool,
+			TINY_DATAMEM_BUF_SIZE, &sdma->tiny_datamem_buf_phys);
+	if (!sdma->tiny_datamem_buf) {
+		sdma->tiny_datamem_buf = dma_alloc_coherent(NULL,
+				TINY_DATAMEM_BUF_SIZE,
+				&sdma->tiny_datamem_buf_phys, GFP_KERNEL);
+		if (!sdma->tiny_datamem_buf) {
+			ret = -ENOMEM;
+			goto err_dma_alloc;
+		}
+	}
+#endif
+
 	sdma->context = (void *)sdma->channel_control +
 		MAX_DMA_CHANNELS * sizeof (struct sdma_channel_control);
 	sdma->context_phys = ccb_phys +
@@ -2150,10 +2163,7 @@

 	/* Set bits of CONFIG register but with static context switching */
 	/* FIXME: Check whether to set ACR bit depending on clock ratios */
-	if (sdma->clk_ratio)
-		writel_relaxed(SDMA_H_CONFIG_ACR, sdma->regs + SDMA_H_CONFIG);
-	else
-		writel_relaxed(0, sdma->regs + SDMA_H_CONFIG);
+	writel_relaxed(0, sdma->regs + SDMA_H_CONFIG);

 	writel_relaxed(ccb_phys, sdma->regs + SDMA_H_C0PTR);

@@ -2180,10 +2190,6 @@

 	if (!imx_dma_is_general_purpose(chan))
 		return false;
-	/* return false if it's not the right device */
-	if ((sdmac->sdma->drvdata == &sdma_imx8m)
-		&& (sdmac->sdma->idx != data->idx))
-		return false;

 	sdmac->data = *data;
 	chan->private = &sdmac->data;
@@ -2205,11 +2211,7 @@

 	data.dma_request = dma_spec->args[0];
 	data.peripheral_type = dma_spec->args[1];
-	/* Get sw_done setting if sw_done enabled */
-	if (dma_spec->args[2] & BIT(31))
-		data.done_sel = dma_spec->args[2];
-	data.priority = dma_spec->args[2] & 0xff;
-	data.idx = sdma->idx;
+	data.priority = dma_spec->args[2];

 	return dma_request_channel(mask, sdma_filter_fn, &data);
 }
@@ -2249,8 +2251,6 @@
 	if (!sdma)
 		return -ENOMEM;

-	sdma->clk_ratio = of_property_read_bool(np, "fsl,ratio-1-1");
-
 	spin_lock_init(&sdma->channel_0_lock);

 	sdma->dev = &pdev->dev;
@@ -2273,26 +2273,19 @@
 	if (IS_ERR(sdma->clk_ahb))
 		return PTR_ERR(sdma->clk_ahb);

-	ret = clk_prepare(sdma->clk_ipg);
-	if (ret)
-		return ret;
-
-	ret = clk_prepare(sdma->clk_ahb);
-	if (ret)
-		goto err_clk;
+	clk_prepare(sdma->clk_ipg);
+	clk_prepare(sdma->clk_ahb);

 	ret = devm_request_irq(&pdev->dev, irq, sdma_int_handler, 0, "sdma",
 			       sdma);
 	if (ret)
-		goto err_irq;
+		return ret;

 	sdma->irq = irq;

 	sdma->script_addrs = kzalloc(sizeof(*sdma->script_addrs), GFP_KERNEL);
-	if (!sdma->script_addrs) {
-		ret = -ENOMEM;
-		goto err_irq;
-	}
+	if (!sdma->script_addrs)
+		return -ENOMEM;

 	/* initially no scripts available */
 	saddr_arr = (s32 *)sdma->script_addrs;
@@ -2314,8 +2307,6 @@
 		sdmac->status = DMA_IN_PROGRESS;
 		sdmac->vc.desc_free = sdma_desc_free;
 		INIT_LIST_HEAD(&sdmac->pending);
-		INIT_WORK(&sdmac->terminate_worker,
-				sdma_channel_terminate_work);

 		/*
 		 * Add the channel to the DMAC list. Do not add channel 0 though
@@ -2324,6 +2315,7 @@
 		 */
 		if (i)
 			vchan_init(&sdmac->vc, &sdma->dma_device);
+
 	}

 	if (np)
@@ -2378,10 +2370,10 @@
 	sdma->dma_device.device_terminate_all = sdma_terminate_all;
 	sdma->dma_device.device_pause = sdma_channel_pause;
 	sdma->dma_device.device_resume = sdma_channel_resume;
-	sdma->dma_device.src_addr_widths = SDMA_DMA_BUSWIDTHS;
-	sdma->dma_device.dst_addr_widths = SDMA_DMA_BUSWIDTHS;
-	sdma->dma_device.directions = SDMA_DMA_DIRECTIONS;
-	sdma->dma_device.residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;
+	sdma->dma_device.src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);
+	sdma->dma_device.dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);
+	sdma->dma_device.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);
+	sdma->dma_device.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;
 	sdma->dma_device.device_prep_dma_memcpy = sdma_prep_memcpy;
 	sdma->dma_device.device_issue_pending = sdma_issue_pending;
 	sdma->dma_device.dev->dma_parms = &sdma->dma_parms;
@@ -2411,19 +2403,14 @@
 		}
 		of_node_put(spba_bus);
 	}
-	/* There maybe multi sdma devices such as i.mx8mscale */
-	sdma->idx = sdma_dev_idx++;
-
+
+	sdma_singleton = sdma;
 	return 0;

 err_register:
 	dma_async_device_unregister(&sdma->dma_device);
 err_init:
 	kfree(sdma->script_addrs);
-err_irq:
-	clk_unprepare(sdma->clk_ahb);
-err_clk:
-	clk_unprepare(sdma->clk_ipg);
 	return ret;
 }

@@ -2435,8 +2422,6 @@
 	devm_free_irq(&pdev->dev, sdma->irq, sdma);
 	dma_async_device_unregister(&sdma->dma_device);
 	kfree(sdma->script_addrs);
-	clk_unprepare(sdma->clk_ahb);
-	clk_unprepare(sdma->clk_ipg);
 	/* Kill the tasklet */
 	for (i = 0; i < MAX_DMA_CHANNELS; i++) {
 		struct sdma_channel *sdmac = &sdma->channel[i];
@@ -2450,36 +2435,13 @@
 }

 #ifdef CONFIG_PM_SLEEP
-static int sdma_save_restore_context(struct sdma_engine *sdma, bool save)
-{
-	struct sdma_context_data *context = sdma->context;
-	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
-	unsigned long flags;
-	int ret;
-
-	spin_lock_irqsave(&sdma->channel_0_lock, flags);
-
-	if (save)
-		bd0->mode.command = C0_GETDM;
-	else
-		bd0->mode.command = C0_SETDM;
-
-	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
-	bd0->mode.count = MAX_DMA_CHANNELS * sizeof(*context) / 4;
-	bd0->buffer_addr = sdma->context_phys;
-	bd0->ext_buffer_addr = 2048;
-	ret = sdma_run_channel0(sdma);
-
-	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
-
-	return ret;
-}
-
 static int sdma_suspend(struct device *dev)
 {
 	struct platform_device *pdev = to_platform_device(dev);
 	struct sdma_engine *sdma = platform_get_drvdata(pdev);
 	int i, ret = 0;
+
+	sdma->suspend_off = false;

 	/* Do nothing if not i.MX6SX or i.MX7D*/
 	if (sdma->drvdata != &sdma_imx6sx && sdma->drvdata != &sdma_imx7d
@@ -2518,7 +2480,6 @@
 {
 	struct platform_device *pdev = to_platform_device(dev);
 	struct sdma_engine *sdma = platform_get_drvdata(pdev);
-	unsigned long timeout = jiffies + msecs_to_jiffies(2);
 	int i, ret;

 	/* Do nothing if not i.MX6SX or i.MX7D*/
@@ -2535,8 +2496,7 @@
 		return 0;
 	}

-	/* Firmware was lost, mark as "not ready" */
-	sdma->fw_loaded = false;
+	sdma->suspend_off = true;

 	/* restore regs and load firmware */
 	for (i = 0; i < MXC_SDMA_SAVED_REG_NUM; i++) {
@@ -2547,10 +2507,6 @@
 		if (i > SDMA_XTRIG_CONF2 / 4)
 			writel_relaxed(sdma->save_regs[i], sdma->regs +
 				       MXC_SDMA_RESERVED_REG + 4 * i);
-		/* set static context switch  mode before channel0 running */
-		else if (i == SDMA_H_CONFIG / 4)
-			writel_relaxed(sdma->save_regs[i] & ~SDMA_H_CONFIG_CSM,
-					sdma->regs + SDMA_H_CONFIG);
 		else
 			writel_relaxed(sdma->save_regs[i] , sdma->regs + 4 * i);
 	}
@@ -2561,31 +2517,108 @@
 	ret = sdma_get_firmware(sdma, sdma->fw_name);
 	if (ret) {
 		dev_warn(&pdev->dev, "failed to get firware\n");
-		goto out;
-	}
-	/* wait firmware loaded */
-	do {
-		if (time_after(jiffies, timeout)) {
-			dev_warn(&pdev->dev, "failed to load firmware\n");
-			break;
-		}
-		usleep_range(50, 500);
-	} while (!sdma->fw_loaded);
+		return ret;
+	}

 	ret = sdma_save_restore_context(sdma, false);
 	if (ret) {
 		dev_err(sdma->dev, "restore context error!\n");
-		goto out;
-	}
-
-	ret = 0;
-out:
+		return ret;
+	}
+
 	clk_disable(sdma->clk_ipg);
 	clk_disable(sdma->clk_ahb);

-	return ret;
+	return 0;
 }
 #endif
+
+struct sdma_channel *sdma_get_channel(struct sdma_engine *sdma, int channel)
+{
+	if (channel < 0 || channel >= MAX_DMA_CHANNELS) {
+		return NULL;
+	}
+	return &sdma->channel[channel];
+}
+EXPORT_SYMBOL(sdma_get_channel);
+
+/* (mjs) convenience function for initializing a channel as
+ * host-triggered or event-triggered
+ * external=false: channel started by host, HO[i]=0, EO[i]=1
+ * external=true: channel started by event, HO[i]=1, EO[i]=0 */
+void sdma_setup_channel(struct sdma_channel *sdmac, bool external)
+{
+	sdma_disable_channel(sdmac);
+	sdma_config_ownership(sdmac,
+		external,   /* event override */
+		!external,  /* host override */
+		false);     /* always false */
+}
+EXPORT_SYMBOL(sdma_setup_channel);
+
+struct sdma_engine *sdma_engine_get(void)
+{
+	return sdma_singleton;
+}
+EXPORT_SYMBOL(sdma_engine_get);
+
+ssize_t sdma_print_context(struct sdma_engine *sdma, int channel, char *buf)
+{
+	static const char *regnames[] = {
+		" r0", " r1", " r2", " r3", " r4", " r5", " r6", " r7",
+		"mda", "msa", " ms", " md", "pda", "psa", " ps", " pd",
+		" ca", " cs", "dda", "dsa", " ds", " dd", "sc0", "sc1",
+		"sc2", "sc3", "sc4", "sc5", "sc6", "sc7"
+	};
+
+	struct sdma_context_data *context;
+	u32 context_addr = sdma_channel_context_base(channel);
+	u32 context_size = sizeof(*context);
+	u32 *regptr;
+	int ret;
+	int i;
+	ssize_t outlen = 0;
+
+	context = kzalloc(context_size, GFP_ATOMIC);
+	if (!context) {
+		return -ENOMEM;
+	}
+
+	ret = sdma_fetch_datamem(sdma, context, context_size, context_addr);
+	if (ret) {
+		return ret;
+	}
+
+	outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen,
+		"pc=%04x rpc=%04x spc=%04x epc=%04x\n",
+		context->channel_state.pc,
+		context->channel_state.rpc,
+		context->channel_state.spc,
+		context->channel_state.epc
+	);
+
+	outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen,
+		"Flags: t=%d sf=%d df=%d lm=%d\n",
+		(context->channel_state.t != 0),
+		(context->channel_state.sf != 0),
+		(context->channel_state.df != 0),
+		(context->channel_state.lm != 0)
+	);
+
+	regptr = &context->gReg[0];
+	for (i = 0; i < ARRAY_SIZE(regnames); i++) {
+		outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen,
+			"%s=%08x%c",
+			regnames[i],
+			regptr[i],
+			((i % 6) == 5) ? '\n' : ' ');
+	}
+	outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen, "\n");
+
+	kfree(context);
+	return outlen;
+}
+EXPORT_SYMBOL(sdma_print_context);

 static const struct dev_pm_ops sdma_pm_ops = {
 	SET_LATE_SYSTEM_SLEEP_PM_OPS(sdma_suspend, sdma_resume)
